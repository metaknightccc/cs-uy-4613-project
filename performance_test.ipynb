{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the performance of the fine-tunning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all the model we need\n",
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "from transformers import BertTokenizerFast, BertForSequenceClassification\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# load the fine-tunning model\n",
    "model_name = \"bert-base-uncased\"\n",
    "model = BertForSequenceClassification.from_pretrained(\"./saved/\")\n",
    "tokenizer = BertTokenizerFast.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8000, 6)\n",
      "(8000,)\n"
     ]
    }
   ],
   "source": [
    "# choose 8000 comments form the training dataset for testing\n",
    "test_num = 8000\n",
    "df = pd.read_csv(\"train.csv\")\n",
    "data = np.array(df)\n",
    "data = data[:,1:]\n",
    "selected_indices = np.random.choice(data.shape[0], test_num, replace=False)\n",
    "data_test = data[selected_indices, :]\n",
    "test_texts = data_test[:, 0]\n",
    "test_labels = data_test[:, 1:]\n",
    "print(test_labels.shape)\n",
    "print(test_texts.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n",
      "2100\n",
      "2200\n",
      "2300\n",
      "2400\n",
      "2500\n",
      "2600\n",
      "2700\n",
      "2800\n",
      "2900\n",
      "3000\n",
      "3100\n",
      "3200\n",
      "3300\n",
      "3400\n",
      "3500\n",
      "3600\n",
      "3700\n",
      "3800\n",
      "3900\n",
      "4000\n",
      "4100\n",
      "4200\n",
      "4300\n",
      "4400\n",
      "4500\n",
      "4600\n",
      "4700\n",
      "4800\n",
      "4900\n",
      "5000\n",
      "5100\n",
      "5200\n",
      "5300\n",
      "5400\n",
      "5500\n",
      "5600\n",
      "5700\n",
      "5800\n",
      "5900\n",
      "6000\n",
      "6100\n",
      "6200\n",
      "6300\n",
      "6400\n",
      "6500\n",
      "6600\n",
      "6700\n",
      "6800\n",
      "6900\n",
      "7000\n",
      "7100\n",
      "7200\n",
      "7300\n",
      "7400\n",
      "7500\n",
      "7600\n",
      "7700\n",
      "7800\n",
      "7900\n"
     ]
    }
   ],
   "source": [
    "# the threshold is 0.5, if the predicted probability>0.5, the label will be 1\n",
    "# mainly test the fine-tunning model's performance by calculating the accuracy and F1 score of the 6 labels\n",
    "threshold = 0.5\n",
    "acc = np.zeros(6)\n",
    "TP = np.zeros(6)\n",
    "FP = np.zeros(6)\n",
    "FN = np.zeros(6)\n",
    "for i in range(test_num):\n",
    "    text = test_texts[i]\n",
    "    y = test_labels[i]\n",
    "    input_val = tokenizer(text, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "    output_val = model(**input_val)\n",
    "    probabilities = torch.sigmoid(output_val.logits)\n",
    "    temp = np.array(probabilities.tolist()[0])\n",
    "    temp = np.where(temp > threshold, 1, 0)\n",
    "    for j in range(6):\n",
    "        if temp[j]==y[j]:\n",
    "            acc[j] += 1\n",
    "        if y[j]==1 and temp[j]==1:\n",
    "            TP[j] += 1\n",
    "        if y[j]==0 and temp[j]==1:\n",
    "            FP[j] += 1\n",
    "        if y[j]==1 and temp[j]==0:\n",
    "            FN[j] += 1\n",
    "    if i % 100 == 0:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7742. 7932. 7906. 7977. 7831. 7930.]\n",
      "[549.  29. 365.   1. 312.   1.]\n",
      "[49.  7. 39.  1. 85.  2.]\n",
      "[209.  61.  55.  22.  84.  68.]\n"
     ]
    }
   ],
   "source": [
    "print(acc)\n",
    "print(TP)\n",
    "print(FP)\n",
    "print(FN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9180602  0.80555556 0.90346535 0.5        0.78589421 0.33333333]\n",
      "[0.72427441 0.32222222 0.86904762 0.04347826 0.78787879 0.01449275]\n"
     ]
    }
   ],
   "source": [
    "P = TP/(TP+FP)\n",
    "R = TP/(TP+FN)\n",
    "print(P)\n",
    "print(R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.96775  0.9915   0.98825  0.997125 0.978875 0.99125 ] [0.80973451 0.46031746 0.88592233 0.08       0.78688525 0.02777778]\n"
     ]
    }
   ],
   "source": [
    "acc = acc/test_num\n",
    "F_1 = 2*(P*R)/(P+R)\n",
    "print(acc, F_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9857916666666666\n",
      "0.5084395545613835\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(acc))\n",
    "print(np.mean(F_1))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall, the fine-tuned model has a good performance. \n",
    "\n",
    "The accuracy of every label, toxic, severe toxic, obscene, insult, identity hate, are 0.96775, 0.9915, 0.98825, 0.997125, 0.978875, 0.99125<br>\n",
    "The average accuracy is 0.9858.\n",
    "\n",
    "The model seems to perform well in terms of overall accuracy. However, accuracy may not always be a reliable metric for imbalanced datasets. Therefore, it is essential to analyze the F1 scores as well.\n",
    "\n",
    "The F1 of every label, toxic, severe toxic, obscene, insult, identity hate, are 0.80973451, 0.46031746, 0.88592233, 0.08, 0.78688525, 0.02777778<br>\n",
    "The average F1 score is 0.5084.\n",
    "- The F1 scores for 'toxic' (0.8097), 'obscene' (0.8859), and 'identity_hate' (0.7869) labels are relatively high, which indicates good performance in these categories.\n",
    "- The F1 scores for 'severe_toxic' (0.4603) and 'insult' (0.0800) labels are relatively low, which indicates the model is not performing well in these categories. The low F1 score for 'severe_toxic' could be due to the imbalanced nature of the dataset or insufficient training data for this category. For the 'insult' category, the low F1 score could be a result of difficulty in distinguishing insults from other types of toxic content.\n",
    "- The F1 score for 'threat' (0.0278) is extremely low, which indicates the model performs poorly in this category. This may be due to the rarity of threat instances in the dataset or a high number of false positives and false negatives.\n",
    "\n",
    "Overall, the fine-tuned model appears to have a good performance in some categories ('toxic', 'obscene', and 'identity_hate') but struggles with others ('severe_toxic', 'insult', and 'threat'). "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
